# twitter_spam_detection

Note: I followed the process https://github.com/pooji0401/Spam-Tweets-Detection to collect and process the dataset. I searched online for a dataset from https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0182487 that contains a series of twitter ids and labels. I downloaded the dataset (I renamed the dataset to original_unpreprocssed_data_with_ids_and_labels.xlsx and included in this repository) and used the hydrator (https://github.com/DocNow/hydrator) with Twitter API to hydrate the dataset to create another dataset that contains the metadata corresponding to the tweet ids from the original downloaded dataset. Hydrate means to extract the tweets' metadata from their corresponding tweet ids. I renamed the original dataset that contains the tweet ids and labels to tweet_spam_label.csv and the dataset that contains the metadata is called tweet_metadata. The training_data.csv is the dataset that is used to train the model after preprocessing (I saved it for reference).

The original downloaded unpreprocessed dataset contained 100,000 tweet ids and labels, but due to the time between the dataset was published and now, many of the tweets may have been deleted, or became private, or the users' account no longer exists, so the hydrated dataset only contains 46,361 tweets. After going through another round of deletion from preprocessing, only 7248 tweets were left to serve as the data to train the model.
